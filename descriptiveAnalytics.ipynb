{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ELITEX V7 - DATA QUALITY & COMPLETENESS ANALYSIS\n",
      "====================================================================================================\n",
      "Analysis Started: 2025-10-16 11:43:35\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import db_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ELITEX V7 - DATA QUALITY & COMPLETENESS ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Analysis Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path(\"Output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Database engine\n",
    "engine = db_engine.elite_engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: GET UNIQUE CLIENTS FROM CLIENT_CONTEXT (BASE TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Loading unique clients from core.client_context...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  Unique clients: 51,179\n",
      "  Columns from client_context: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 1: Loading unique clients from core.client_context...\")\nprint(\"-\" * 100)\n\nquery_clients = \"\"\"\nSELECT * FROM core.client_context\nWHERE client_id IS NOT NULL\n\"\"\"\n\ndf_clients = pd.read_sql(query_clients, engine)\ndf_clients = df_clients.drop_duplicates(subset=['client_id'])\ndf_clients['client_id'] = df_clients['client_id'].str.upper()\n\ntotal_clients = len(df_clients)\nprint(f\"\u2713 Loaded {total_clients:,} unique clients\")\nprint(f\"  Total columns from client_context: {len(df_clients.columns)}\")\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD CLIENT_INVESTMENT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Loading core.client_investment...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u2713 Loaded 1,955 investment records\n",
      "  Unique clients with investments: 944\n",
      "  Aggregated to client level: 944 rows\n",
      "  client_id portfolio_id  total_cost_value_aed  total_market_value_aed  \\\n",
      "0   10GLPHG   1019461-41           3700000.000              3700000.00   \n",
      "1   10GRRXX   1013322-31           5000000.000              5197900.00   \n",
      "2   10PKFPQ   1047540-31             40478.113                41909.19   \n",
      "3   10QAXPK   1008247-31            185812.700               137010.51   \n",
      "4   10QHKPA   1006748-31             97864.257               149971.35   \n",
      "5   10QHLHP   1006964-31              4080.703                 3085.32   \n",
      "6   10QXGLF   1002195-31            517158.400                98011.80   \n",
      "7   10QXRPX   1002342-31             92014.527               145161.74   \n",
      "8   10RXAKP   1032874-31            487041.563               374783.92   \n",
      "9   10XQKRF   1020735-31            560594.012               504302.02   \n",
      "\n",
      "   portfolio_xirr  investment_holdings_count  \n",
      "0        0.000000                          1  \n",
      "1        0.039580                          1  \n",
      "2        0.035354                          1  \n",
      "3       -0.262642                          4  \n",
      "4        0.532443                          2  \n",
      "5       -0.243924                          1  \n",
      "6       -0.810480                          2  \n",
      "7        0.577596                          4  \n",
      "8       -0.230489                          2  \n",
      "9       -0.100415                          3  \n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: Loading core.client_investment...\")\nprint(\"-\" * 100)\n\nquery_investment = \"\"\"\nSELECT * FROM core.client_investment\n\"\"\"\n\ntry:\n    df_investment = pd.read_sql(query_investment, engine)\n    df_investment['client_id'] = df_investment['client_id'].str.upper()\n    \n    # Aggregate by client - for numeric columns sum, for others take first\n    numeric_cols = df_investment.select_dtypes(include=['number']).columns.tolist()\n    numeric_cols = [col for col in numeric_cols if col != 'client_id']\n    \n    agg_dict = {col: 'sum' for col in numeric_cols}\n    for col in df_investment.columns:\n        if col not in numeric_cols and col != 'client_id':\n            agg_dict[col] = 'first'\n    \n    df_investment_agg = df_investment.groupby('client_id').agg(agg_dict).reset_index()\n    df_investment_agg.columns = ['client_id'] + [f'inv_{col}' for col in df_investment_agg.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_investment):,} investment records with {len(df_investment.columns)} columns\")\n    print(f\"  Unique clients with investments: {df_investment['client_id'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_investment_agg)} clients with {len(df_investment_agg.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading client_investment: {e}\")\n    df_investment_agg = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: LOAD CLIENT_PORTFOLIO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Loading core.client_portfolio...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u2713 Loaded 1,958 portfolio records\n",
      "  Unique clients with portfolio: 1,956\n",
      "  client_id last_valuation_date        aum  investible_cash deposits  \\\n",
      "0   10FGALK                None        0.0         30449.68     None   \n",
      "1   10GGLFG                None        0.0        248006.80     None   \n",
      "2   10GLFHG                None        0.0           152.39     None   \n",
      "3   10GLPHG                None  3700000.0             0.33     None   \n",
      "4   10GPKGQ                None        0.0        297681.30     None   \n",
      "\n",
      "  asset_distribution  \n",
      "0               None  \n",
      "1               None  \n",
      "2               None  \n",
      "3               None  \n",
      "4               None  \n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 3: Loading core.client_portfolio...\")\nprint(\"-\" * 100)\n\nquery_portfolio = \"\"\"\nSELECT * FROM core.client_portfolio\n\"\"\"\n\ntry:\n    df_portfolio = pd.read_sql(query_portfolio, engine)\n    df_portfolio['client_id'] = df_portfolio['client_id'].str.upper()\n    \n    # Take most recent record per client (if date column exists, otherwise take first)\n    if 'last_valuation_date' in df_portfolio.columns:\n        df_portfolio_latest = df_portfolio.sort_values('last_valuation_date', ascending=False).groupby('client_id').first().reset_index()\n    else:\n        df_portfolio_latest = df_portfolio.groupby('client_id').first().reset_index()\n    \n    # Rename columns to avoid conflicts\n    df_portfolio_latest.columns = ['client_id'] + [f'port_{col}' for col in df_portfolio_latest.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_portfolio):,} portfolio records with {len(df_portfolio.columns)} columns\")\n    print(f\"  Unique clients with portfolio: {df_portfolio['client_id'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_portfolio_latest)} clients with {len(df_portfolio_latest.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading client_portfolio: {e}\")\n    df_portfolio_latest = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: LOAD PRODUCTBALANCE (using customer_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: Loading core.productbalance (maps to customer_number)...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u2713 Loaded 113,028 product balance records\n",
      "  Unique customers with products: 44,156\n",
      "  client_id  total_outstanding_balance  product_count  \\\n",
      "0   10AAAKQ                     374.46              1   \n",
      "1   10AAFXP                 -777953.17              3   \n",
      "2   10AAGQH                 3695046.14              3   \n",
      "3   10AAHAH                -1112816.39              3   \n",
      "4   10AAKAX                   81077.37              1   \n",
      "\n",
      "                        product_types  \n",
      "0                    DEPOSIT PRODUCTS  \n",
      "1  LENDING PRODUCTS, DEPOSIT PRODUCTS  \n",
      "2  DEPOSIT PRODUCTS, LENDING PRODUCTS  \n",
      "3  DEPOSIT PRODUCTS, LENDING PRODUCTS  \n",
      "4                    DEPOSIT PRODUCTS  \n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 4: Loading core.productbalance (maps to customer_number)...\")\nprint(\"-\" * 100)\n\nquery_productbalance = \"\"\"\nSELECT * FROM core.productbalance\n\"\"\"\n\ntry:\n    df_productbalance = pd.read_sql(query_productbalance, engine)\n    df_productbalance['customer_number'] = df_productbalance['customer_number'].str.upper()\n    \n    # Aggregate numeric columns by customer\n    numeric_cols = df_productbalance.select_dtypes(include=['number']).columns.tolist()\n    agg_dict = {col: 'sum' for col in numeric_cols}\n    agg_dict['account_number'] = 'count'  # count accounts\n    \n    for col in df_productbalance.columns:\n        if col not in numeric_cols and col != 'customer_number' and col != 'account_number':\n            if col in agg_dict:\n                continue\n            agg_dict[col] = 'first'\n    \n    df_pb_agg = df_productbalance.groupby('customer_number').agg(agg_dict).reset_index()\n    df_pb_agg.columns = ['client_id'] + [f'pb_{col}' for col in df_pb_agg.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_productbalance):,} product balance records with {len(df_productbalance.columns)} columns\")\n    print(f\"  Unique customers with products: {df_productbalance['customer_number'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_pb_agg)} clients with {len(df_pb_agg.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading productbalance: {e}\")\n    df_pb_agg = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: LOAD CLIENT_PROD_BALANCE_MONTHLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: Loading core.client_prod_balance_monthly...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u2713 Loaded 262,212 monthly balance records\n",
      "  Unique clients with monthly data: 44,536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 5: Loading core.client_prod_balance_monthly...\")\nprint(\"-\" * 100)\n\nquery_monthly = \"\"\"\nSELECT * FROM core.client_prod_balance_monthly\n\"\"\"\n\ntry:\n    df_monthly = pd.read_sql(query_monthly, engine)\n    df_monthly['client_id'] = df_monthly['client_id'].str.upper()\n    \n    # Get most recent month per client\n    if 'year_cal' in df_monthly.columns and 'month_cal' in df_monthly.columns:\n        df_monthly['year_cal'] = pd.to_numeric(df_monthly['year_cal'], errors='coerce')\n        df_monthly['month_cal'] = pd.to_numeric(df_monthly['month_cal'], errors='coerce')\n        df_monthly = df_monthly.sort_values(['year_cal', 'month_cal'], ascending=False)\n    \n    df_monthly_latest = df_monthly.groupby('client_id').first().reset_index()\n    df_monthly_latest.columns = ['client_id'] + [f'monthly_{col}' for col in df_monthly_latest.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_monthly):,} monthly balance records with {len(df_monthly.columns)} columns\")\n    print(f\"  Unique clients with monthly data: {df_monthly['client_id'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_monthly_latest)} clients with {len(df_monthly_latest.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading client_prod_balance_monthly: {e}\")\n    df_monthly_latest = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: Loading core.aecbalerts (maps to cif)...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                               record_id              id      cif        cif2  \\\n",
      "0   a670eb70-4100-4113-8184-e237c7a2b97f  20240801003503  39QFGGR  80RHHHXGXH   \n",
      "1   40f0e2e1-3ae4-4676-9534-18953584c39d  20240801003503  60PHGFH        None   \n",
      "2   3e399b12-71ba-4f6a-8880-8f96701b46d9  20240801003503  12KGQAR     36KPKKH   \n",
      "3   fa841029-becd-4bee-98e3-3dc4b70f897b  20240801003503  16QAFKF        None   \n",
      "4   dc090681-4325-4f51-9d5e-d11113662e3f  20240801003503  44RGLKK        None   \n",
      "..                                   ...             ...      ...         ...   \n",
      "95  9f027bd1-a77a-4fa6-9261-c76cc0a69f9b  20240801003503  46QGXLA        None   \n",
      "96  f0564d42-0e9a-43ea-b180-4523f463d07e  20240801003503  11LGFPX     38KKLXP   \n",
      "97  f9f66785-84e7-4407-a0c6-9b27ad8dd990  20240801003503  41QAPRP        None   \n",
      "98  f0b19dcf-6160-475c-b2e6-e6fd32292010  20240801003503  10XQHXR     80ARQGR   \n",
      "99  1ceb7d1b-2a92-4112-b9ab-f6d9759c111d  20240801003503  10XQHXR     80ARQGR   \n",
      "\n",
      "   role  balance  number1  category  opendate  recordid  ...  \\\n",
      "0     A      NaN     15.0       1.0       NaN   10262.0  ...   \n",
      "1     A      NaN     15.0       1.0       NaN   10670.0  ...   \n",
      "2     A      NaN     15.0       1.0       NaN   10751.0  ...   \n",
      "3     A      NaN     15.0       1.0       NaN   10761.0  ...   \n",
      "4     A      NaN     15.0       1.0       NaN   10773.0  ...   \n",
      "..  ...      ...      ...       ...       ...       ...  ...   \n",
      "95    A      NaN     15.0       1.0       NaN    9023.0  ...   \n",
      "96    A      NaN     15.0       1.0       NaN    9118.0  ...   \n",
      "97    A      NaN     15.0       1.0       NaN    9132.0  ...   \n",
      "98    A      NaN     15.0       1.0       NaN    9450.0  ...   \n",
      "99    A      NaN     15.0       1.0       NaN    9451.0  ...   \n",
      "\n",
      "     extractdatetime directdebitamount contractstatusdate  \\\n",
      "0   0108202400:35:03              None               None   \n",
      "1   0108202400:35:03              None               None   \n",
      "2   0108202400:35:03              None               None   \n",
      "3   0108202400:35:03              None               None   \n",
      "4   0108202400:35:03              None               None   \n",
      "..               ...               ...                ...   \n",
      "95  0108202400:35:03              None               None   \n",
      "96  0108202400:35:03              None               None   \n",
      "97  0108202400:35:03              None               None   \n",
      "98  0108202400:35:03              None               None   \n",
      "99  0108202400:35:03              None               None   \n",
      "\n",
      "    bouncedchequeamount salarycreditedamount suspiciousactivityflagdate  \\\n",
      "0                  None                 None                       None   \n",
      "1                  None                 None                       None   \n",
      "2                  None                 None                       None   \n",
      "3                  None                 None                       None   \n",
      "4                  None                 None                       None   \n",
      "..                  ...                  ...                        ...   \n",
      "95                 None                 None                       None   \n",
      "96                 None                 None                       None   \n",
      "97                 None                 None                       None   \n",
      "98                 None                 None                       None   \n",
      "99                 None                 None                       None   \n",
      "\n",
      "                            file_source                creation_ts  \\\n",
      "0   20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "1   20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "2   20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "3   20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "4   20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "..                                  ...                        ...   \n",
      "95  20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "96  20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "97  20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "98  20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "99  20250228/aecb_alerts/ALERTS.parquet 2025-07-16 13:45:02.834215   \n",
      "\n",
      "                      load_ts               last_updated  \n",
      "0  2025-07-16 13:45:02.891421 2025-07-16 16:53:40.338746  \n",
      "1  2025-07-16 13:45:02.891567 2025-07-16 16:53:40.338746  \n",
      "2  2025-07-16 13:45:02.891630 2025-07-16 16:53:40.338746  \n",
      "3  2025-07-16 13:45:02.891686 2025-07-16 16:53:40.338746  \n",
      "4  2025-07-16 13:45:02.891742 2025-07-16 16:53:40.338746  \n",
      "..                        ...                        ...  \n",
      "95 2025-07-16 13:45:02.896133 2025-07-16 16:53:40.338746  \n",
      "96 2025-07-16 13:45:02.896180 2025-07-16 16:53:40.338746  \n",
      "97 2025-07-16 13:45:02.896228 2025-07-16 16:53:40.338746  \n",
      "98 2025-07-16 13:45:02.896275 2025-07-16 16:53:40.338746  \n",
      "99 2025-07-16 13:45:02.896324 2025-07-16 16:53:40.338746  \n",
      "\n",
      "[100 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 6: Loading core.aecbalerts (maps to cif)...\")\nprint(\"-\" * 100)\n\nquery_aecb = \"\"\"\nSELECT * FROM core.aecbalerts\n\"\"\"\n\ntry:\n    df_aecb = pd.read_sql(query_aecb, engine)\n    df_aecb['cif'] = df_aecb['cif'].str.upper()\n    \n    # Aggregate numeric columns\n    numeric_cols = df_aecb.select_dtypes(include=['number']).columns.tolist()\n    agg_dict = {col: 'sum' for col in numeric_cols}\n    \n    for col in df_aecb.columns:\n        if col not in numeric_cols and col != 'cif':\n            agg_dict[col] = 'first'\n    \n    df_aecb_agg = df_aecb.groupby('cif').agg(agg_dict).reset_index()\n    df_aecb_agg.columns = ['client_id'] + [f'aecb_{col}' for col in df_aecb_agg.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_aecb):,} AECB alert records with {len(df_aecb.columns)} columns\")\n    print(f\"  Unique clients with AECB alerts: {df_aecb['cif'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_aecb_agg)} clients with {len(df_aecb_agg.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading aecbalerts: {e}\")\n    df_aecb_agg = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: LOAD AECB ALERTS (using cif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: Loading core.aecbalerts (maps to cif)...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u2713 Loaded 29,168 AECB alert records\n",
      "  Unique clients with AECB alerts: 9,638\n",
      "          client_id  aecb_total_amount  aecb_overdue_amount  aecb_alerts_count\n",
      "0  00QQQQQQQQXGPLQQ                0.0                  0.0                  2\n",
      "1           10AAFXP                0.0                  0.0                  1\n",
      "2           10AALLX                0.0                  0.0                  4\n",
      "3           10AGAFG                0.0                  0.0                  2\n",
      "4           10AGAHP                0.0                 34.0                  4\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 6: Loading core.aecbalerts (maps to cif)...\")\nprint(\"-\" * 100)\n\nquery_aecb = \"\"\"\nSELECT * FROM core.aecbalerts\n\"\"\"\n\ntry:\n    df_aecb = pd.read_sql(query_aecb, engine)\n    df_aecb['cif'] = df_aecb['cif'].str.upper()\n    \n    # Aggregate numeric columns\n    numeric_cols = df_aecb.select_dtypes(include=['number']).columns.tolist()\n    agg_dict = {col: 'sum' for col in numeric_cols}\n    \n    for col in df_aecb.columns:\n        if col not in numeric_cols and col != 'cif':\n            agg_dict[col] = 'first'\n    \n    df_aecb_agg = df_aecb.groupby('cif').agg(agg_dict).reset_index()\n    df_aecb_agg.columns = ['client_id'] + [f'aecb_{col}' for col in df_aecb_agg.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_aecb):,} AECB alert records with {len(df_aecb.columns)} columns\")\n    print(f\"  Unique clients with AECB alerts: {df_aecb['cif'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_aecb_agg)} clients with {len(df_aecb_agg.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading aecbalerts: {e}\")\n    df_aecb_agg = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: LOAD DEBIT TRANSACTIONS (using cif2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 7: Skipping core.clienttransactiondebit (column mapping issues)...\")\nprint(\"-\" * 100)\ndf_debit_agg = pd.DataFrame(columns=['client_id'])\nprint(\"\u2713 Skipped\")\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: LOAD CREDIT TRANSACTIONS (using cif2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 8: Skipping core.clienttransactioncredit (column mapping issues)...\")\nprint(\"-\" * 100)\ndf_credit_agg = pd.DataFrame(columns=['client_id'])\nprint(\"\u2713 Skipped\")\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: LOAD BANCASSURANCE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 9: Loading core.bancaclientproduct...\")\nprint(\"-\" * 100)\n\nquery_banca = \"\"\"\nSELECT * FROM core.bancaclientproduct\n\"\"\"\n\ntry:\n    df_banca = pd.read_sql(query_banca, engine)\n    df_banca['client_id'] = df_banca['client_id'].str.upper()\n    \n    # Aggregate by client\n    numeric_cols = df_banca.select_dtypes(include=['number']).columns.tolist()\n    agg_dict = {col: 'sum' for col in numeric_cols}\n    agg_dict['policy_number'] = 'count'  # count policies\n    \n    for col in df_banca.columns:\n        if col not in numeric_cols and col != 'client_id' and col != 'policy_number':\n            if col in agg_dict:\n                continue\n            agg_dict[col] = 'first'\n    \n    df_banca_agg = df_banca.groupby('client_id').agg(agg_dict).reset_index()\n    df_banca_agg.columns = ['client_id'] + [f'banca_{col}' for col in df_banca_agg.columns[1:]]\n    \n    print(f\"\u2713 Loaded {len(df_banca):,} bancassurance records with {len(df_banca.columns)} columns\")\n    print(f\"  Unique clients with bancassurance: {df_banca['client_id'].nunique():,}\")\n    print(f\"  Aggregated to: {len(df_banca_agg)} clients with {len(df_banca_agg.columns)} columns\")\nexcept Exception as e:\n    print(f\"\u2717 Error loading bancaclientproduct: {e}\")\n    df_banca_agg = pd.DataFrame(columns=['client_id'])\n\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: LOAD UPSELL OPPORTUNITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 1: Loading unique clients from core.client_context...\")\nprint(\"-\" * 100)\n\nquery_clients = \"\"\"\nSELECT * FROM core.client_context\nWHERE client_id IS NOT NULL\n\"\"\"\n\ndf_clients = pd.read_sql(query_clients, engine)\ndf_clients = df_clients.drop_duplicates(subset=['client_id'])\ndf_clients['client_id'] = df_clients['client_id'].str.upper()\n\ntotal_clients = len(df_clients)\nprint(f\"\u2713 Loaded {total_clients:,} unique clients\")\nprint(f\"  Total columns from client_context: {len(df_clients.columns)}\")\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: LOAD USER JOIN CLIENT CONTEXT (RM MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 1: Loading unique clients from core.client_context...\")\nprint(\"-\" * 100)\n\nquery_clients = \"\"\"\nSELECT * FROM core.client_context\nWHERE client_id IS NOT NULL\n\"\"\"\n\ndf_clients = pd.read_sql(query_clients, engine)\ndf_clients = df_clients.drop_duplicates(subset=['client_id'])\ndf_clients['client_id'] = df_clients['client_id'].str.upper()\n\ntotal_clients = len(df_clients)\nprint(f\"\u2713 Loaded {total_clients:,} unique clients\")\nprint(f\"  Total columns from client_context: {len(df_clients.columns)}\")\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: MERGE ALL DATA USING LEFT JOIN ON CLIENT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 12: MERGING ALL DATA TABLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Start with base clients table\n",
    "df_merged = df_clients.copy()\n",
    "print(f\"Starting with base: {len(df_merged):,} clients, {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge investment data\n",
    "if not df_investment_agg.empty:\n",
    "    df_merged = df_merged.merge(df_investment_agg, on='client_id', how='left')\n",
    "    print(f\"After merging investment: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge portfolio data\n",
    "if not df_portfolio_latest.empty:\n",
    "    df_merged = df_merged.merge(df_portfolio_latest, on='client_id', how='left')\n",
    "    print(f\"After merging portfolio: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge product balance\n",
    "if not df_pb_agg.empty:\n",
    "    df_merged = df_merged.merge(df_pb_agg, on='client_id', how='left')\n",
    "    print(f\"After merging product balance: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge monthly balance\n",
    "if not df_monthly_latest.empty:\n",
    "    df_merged = df_merged.merge(df_monthly_latest, on='client_id', how='left')\n",
    "    print(f\"After merging monthly balance: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge AECB alerts\n",
    "if not df_aecb_agg.empty:\n",
    "    df_merged = df_merged.merge(df_aecb_agg, on='client_id', how='left')\n",
    "    print(f\"After merging AECB alerts: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge debit transactions\n",
    "if not df_debit_agg.empty:\n",
    "    df_merged = df_merged.merge(df_debit_agg, on='client_id', how='left')\n",
    "    print(f\"After merging debit transactions: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge credit transactions\n",
    "if not df_credit_agg.empty:\n",
    "    df_merged = df_merged.merge(df_credit_agg, on='client_id', how='left')\n",
    "    print(f\"After merging credit transactions: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge bancassurance\n",
    "if not df_banca_agg.empty:\n",
    "    df_merged = df_merged.merge(df_banca_agg, on='client_id', how='left')\n",
    "    print(f\"After merging bancassurance: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge upsell opportunities\n",
    "if not df_upsell_agg.empty:\n",
    "    df_merged = df_merged.merge(df_upsell_agg, on='client_id', how='left')\n",
    "    print(f\"After merging upsell opportunities: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge RM mapping\n",
    "if not df_rm_unique.empty:\n",
    "    df_merged = df_merged.merge(df_rm_unique, on='client_id', how='left')\n",
    "    print(f\"After merging RM mapping: {len(df_merged.columns)} columns\")\n",
    "\n",
    "print(f\"\\n\u2713 Final merged dataset: {len(df_merged):,} clients, {len(df_merged.columns)} total columns\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 13: ANALYZE DATA COVERAGE AND MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 13: ANALYZING DATA COVERAGE AND MISSING VALUES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate missing values per column\n",
    "missing_analysis = []\n",
    "\n",
    "for col in df_merged.columns:\n",
    "    total_count = len(df_merged)\n",
    "    non_null_count = df_merged[col].notna().sum()\n",
    "    null_count = total_count - non_null_count\n",
    "    coverage_pct = (non_null_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    missing_analysis.append({\n",
    "        'column_name': col,\n",
    "        'total_clients': total_count,\n",
    "        'clients_with_data': non_null_count,\n",
    "        'clients_missing_data': null_count,\n",
    "        'coverage_pct': round(coverage_pct, 2),\n",
    "        'missing_pct': round(100 - coverage_pct, 2)\n",
    "    })\n",
    "\n",
    "df_coverage = pd.DataFrame(missing_analysis)\n",
    "df_coverage = df_coverage.sort_values('coverage_pct', ascending=True)\n",
    "\n",
    "print(f\"\u2713 Analyzed {len(df_coverage)} columns\")\n",
    "print(f\"  Columns with 100% coverage: {len(df_coverage[df_coverage['coverage_pct'] == 100])}\")\n",
    "print(f\"  Columns with <50% coverage: {len(df_coverage[df_coverage['coverage_pct'] < 50])}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 14: CATEGORIZE COLUMNS BY SOURCE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 14: Categorizing columns by source table...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Map columns to source tables\n",
    "column_source_map = []\n",
    "\n",
    "# Client context columns (base table)\n",
    "for col in df_clients.columns:\n",
    "    column_source_map.append({\n",
    "        'column_name': col,\n",
    "        'source_table': 'core.client_context',\n",
    "        'is_base_table': True\n",
    "    })\n",
    "\n",
    "# Investment columns\n",
    "for col in df_investment_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.client_investment',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Portfolio columns\n",
    "for col in df_portfolio_latest.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.client_portfolio',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Product balance columns\n",
    "for col in df_pb_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.productbalance',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Monthly balance columns\n",
    "for col in df_monthly_latest.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.client_prod_balance_monthly',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# AECB columns\n",
    "for col in df_aecb_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.aecbalerts',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Debit transaction columns\n",
    "for col in df_debit_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.clienttransactiondebit',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Credit transaction columns\n",
    "for col in df_credit_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.clienttransactioncredit',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Bancassurance columns\n",
    "for col in df_banca_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.bancaclientproduct',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Upsell columns\n",
    "for col in df_upsell_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'app.upsellopportunity',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# RM mapping columns\n",
    "for col in df_rm_unique.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.user_join_client_context',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "df_column_sources = pd.DataFrame(column_source_map)\n",
    "\n",
    "# Merge with coverage analysis\n",
    "df_coverage_detailed = df_coverage.merge(df_column_sources, on='column_name', how='left')\n",
    "df_coverage_detailed['source_table'] = df_coverage_detailed['source_table'].fillna('Unknown')\n",
    "\n",
    "print(f\"\u2713 Categorized {len(df_coverage_detailed)} columns by source table\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 15: GENERATE SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 15: Generating summary statistics...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "summary_stats = {\n",
    "    'total_unique_clients': total_clients,\n",
    "    'total_columns_in_merged_data': len(df_merged.columns),\n",
    "    'columns_from_client_context': len([c for c in df_coverage_detailed['source_table'] if c == 'core.client_context']),\n",
    "    'columns_with_100pct_coverage': len(df_coverage[df_coverage['coverage_pct'] == 100]),\n",
    "    'columns_with_50_to_100pct_coverage': len(df_coverage[(df_coverage['coverage_pct'] >= 50) & (df_coverage['coverage_pct'] < 100)]),\n",
    "    'columns_with_less_than_50pct_coverage': len(df_coverage[df_coverage['coverage_pct'] < 50]),\n",
    "    'average_coverage_pct': round(df_coverage['coverage_pct'].mean(), 2),\n",
    "    'median_coverage_pct': round(df_coverage['coverage_pct'].median(), 2),\n",
    "}\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"-\" * 100)\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 16: IDENTIFY KEY MISSING DATA ISSUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 16: Identifying key missing data issues...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Find columns with significant missing data (>50% missing)\n",
    "high_missing = df_coverage_detailed[df_coverage_detailed['missing_pct'] > 50].sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(f\"Columns with >50% missing data: {len(high_missing)}\")\n",
    "if len(high_missing) > 0:\n",
    "    print(\"\\nTop 10 columns with highest missing data:\")\n",
    "    print(\"-\" * 100)\n",
    "    for idx, row in high_missing.head(10).iterrows():\n",
    "        print(f\"  {row['column_name']:<40} {row['source_table']:<40} {row['missing_pct']:.1f}% missing\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 17: EXPORT ALL RESULTS TO EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\nprint(\"STEP 17: EXPORTING RESULTS TO EXCEL\")\nprint(\"=\"*100)\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\noutput_file = OUTPUT_DIR / f\"EliteX_Data_Quality_Report_{timestamp}.xlsx\"\n\nprint(\"Creating comprehensive Excel report with all data tables...\")\nprint(\"-\" * 100)\n\n# Create table summary before writing\ntable_summary = df_coverage_detailed.groupby('source_table').agg({\n    'column_name': 'count',\n    'coverage_pct': 'mean'\n}).reset_index()\ntable_summary.columns = ['source_table', 'column_count', 'avg_coverage_pct']\ntable_summary = table_summary.sort_values('avg_coverage_pct', ascending=False)\n\nwith pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n    \n    # Sheet 1: Executive Summary\n    df_summary = pd.DataFrame([summary_stats]).T\n    df_summary.columns = ['Value']\n    df_summary.index.name = 'Metric'\n    df_summary.to_excel(writer, sheet_name='Executive Summary')\n    print(\"\u2713 Sheet 1: Executive Summary\")\n    \n    # Sheet 2: Merged Data (first 10,000 rows)\n    df_merged.head(10000).to_excel(writer, sheet_name='Merged Data (Sample)', index=False)\n    print(f\"\u2713 Sheet 2: Merged Data (Sample - first {min(10000, len(df_merged))} rows)\")\n    \n    # Sheet 3: Column Coverage Analysis\n    df_coverage_detailed.to_excel(writer, sheet_name='Column Coverage', index=False)\n    print(f\"\u2713 Sheet 3: Column Coverage ({len(df_coverage_detailed)} columns)\")\n    \n    # Sheet 4: High Missing Data\n    if not high_missing.empty:\n        high_missing.to_excel(writer, sheet_name='High Missing Data', index=False)\n        print(f\"\u2713 Sheet 4: High Missing Data ({len(high_missing)} columns)\")\n    \n    # Sheet 5: Coverage by Table\n    table_summary.to_excel(writer, sheet_name='Coverage by Table', index=False)\n    print(f\"\u2713 Sheet 5: Coverage by Table ({len(table_summary)} tables)\")\n    \n    # NEW: Export all raw tables with ALL columns\n    print(\"\\nExporting raw data tables...\")\n    print(\"-\" * 100)\n    \n    # Client Context (full data)\n    if not df_clients.empty:\n        df_clients.to_excel(writer, sheet_name='RAW_ClientContext', index=False)\n        print(f\"\u2713 RAW_ClientContext: {len(df_clients):,} rows, {len(df_clients.columns)} columns\")\n    \n    # Investment (full raw data, not aggregated)\n    if 'df_investment' in dir() and not df_investment.empty:\n        df_investment.to_excel(writer, sheet_name='RAW_Investment', index=False)\n        print(f\"\u2713 RAW_Investment: {len(df_investment):,} rows, {len(df_investment.columns)} columns\")\n    \n    # Portfolio (full raw data)\n    if 'df_portfolio' in dir() and not df_portfolio.empty:\n        df_portfolio.to_excel(writer, sheet_name='RAW_Portfolio', index=False)\n        print(f\"\u2713 RAW_Portfolio: {len(df_portfolio):,} rows, {len(df_portfolio.columns)} columns\")\n    \n    # Product Balance (full raw data)\n    if 'df_productbalance' in dir() and not df_productbalance.empty:\n        # Limit to 100k rows for Excel size\n        df_pb_export = df_productbalance.head(100000)\n        df_pb_export.to_excel(writer, sheet_name='RAW_ProductBalance', index=False)\n        print(f\"\u2713 RAW_ProductBalance: {len(df_pb_export):,} rows (limited), {len(df_pb_export.columns)} columns\")\n    \n    # Monthly Balance (full raw data)\n    if 'df_monthly' in dir() and not df_monthly.empty:\n        # Limit to 50k rows\n        df_monthly_export = df_monthly.head(50000)\n        df_monthly_export.to_excel(writer, sheet_name='RAW_MonthlyBalance', index=False)\n        print(f\"\u2713 RAW_MonthlyBalance: {len(df_monthly_export):,} rows (limited), {len(df_monthly_export.columns)} columns\")\n    \n    # AECB Alerts (full raw data)\n    if 'df_aecb' in dir() and not df_aecb.empty:\n        df_aecb.to_excel(writer, sheet_name='RAW_AECB_Alerts', index=False)\n        print(f\"\u2713 RAW_AECB_Alerts: {len(df_aecb):,} rows, {len(df_aecb.columns)} columns\")\n    \n    # Debit Transactions (sample due to size)\n    if 'df_debit' in dir() and not df_debit.empty:\n        df_debit_export = df_debit.head(50000)\n        df_debit_export.to_excel(writer, sheet_name='RAW_DebitTxn', index=False)\n        print(f\"\u2713 RAW_DebitTxn: {len(df_debit_export):,} rows (limited), {len(df_debit_export.columns)} columns\")\n    \n    # Credit Transactions (sample due to size)\n    if 'df_credit' in dir() and not df_credit.empty:\n        df_credit_export = df_credit.head(50000)\n        df_credit_export.to_excel(writer, sheet_name='RAW_CreditTxn', index=False)\n        print(f\"\u2713 RAW_CreditTxn: {len(df_credit_export):,} rows (limited), {len(df_credit_export.columns)} columns\")\n    \n    # Bancassurance (full raw data)\n    if 'df_banca' in dir() and not df_banca.empty:\n        df_banca.to_excel(writer, sheet_name='RAW_Bancassurance', index=False)\n        print(f\"\u2713 RAW_Bancassurance: {len(df_banca):,} rows, {len(df_banca.columns)} columns\")\n    \n    # Upsell (full raw data)\n    if 'df_upsell' in dir() and not df_upsell.empty:\n        df_upsell.to_excel(writer, sheet_name='RAW_Upsell', index=False)\n        print(f\"\u2713 RAW_Upsell: {len(df_upsell):,} rows, {len(df_upsell.columns)} columns\")\n    \n    # RM Mapping (full raw data)\n    if 'df_rm' in dir() and not df_rm.empty:\n        df_rm.to_excel(writer, sheet_name='RAW_RM_Mapping', index=False)\n        print(f\"\u2713 RAW_RM_Mapping: {len(df_rm):,} rows, {len(df_rm.columns)} columns\")\n\nprint(\"\\n\" + \"=\"*100)\nprint(f\"\u2713 Excel report saved: {output_file}\")\nprint(\"=\"*100)\nprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 18: PRINT FINAL SUMMARY TO CONSOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nTotal Unique Clients: {total_clients:,}\")\n",
    "print(f\"Total Columns in Merged Dataset: {len(df_merged.columns)}\")\n",
    "print(f\"Average Column Coverage: {summary_stats['average_coverage_pct']}%\")\n",
    "print(f\"Columns with <50% Coverage: {summary_stats['columns_with_less_than_50pct_coverage']}\")\n",
    "\n",
    "print(f\"\\n\u2713 Full report saved to: {output_file}\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"Analysis finished successfully!\")\n",
    "print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}