{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import db_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ELITEX V7 - DATA QUALITY & COMPLETENESS ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Analysis Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path(\"Output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Database engine\n",
    "engine = db_engine.elite_engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 1: GET UNIQUE CLIENTS FROM CLIENT_CONTEXT (BASE TABLE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 1: Loading unique clients from core.client_context...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_clients = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    client_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    employer,\n",
    "    dob,\n",
    "    age,\n",
    "    gender,\n",
    "    occupation,\n",
    "    education,\n",
    "    family,\n",
    "    income,\n",
    "    occupation_sector,\n",
    "    customer_personal_nationality,\n",
    "    customer_personal_residence,\n",
    "    customer_profile_banking_segment,\n",
    "    customer_profile_subsegment,\n",
    "    emirate,\n",
    "    communication_no_1,\n",
    "    communication_type_1,\n",
    "    communication_no_2,\n",
    "    communication_type_2,\n",
    "    email,\n",
    "    client_off_us_relationships,\n",
    "    client_off_us_relationship_bank,\n",
    "    risk_appetite,\n",
    "    risk_level,\n",
    "    risk_segment,\n",
    "    open_date,\n",
    "    tenure,\n",
    "    kyc_date,\n",
    "    kyc_expiry_date,\n",
    "    professional_investor_flag,\n",
    "    aecb_rating,\n",
    "    client_picture,\n",
    "    last_update\n",
    "FROM core.client_context\n",
    "WHERE client_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "df_clients = pd.read_sql(query_clients, engine)\n",
    "df_clients = df_clients.drop_duplicates(subset=['client_id'])\n",
    "df_clients['client_id'] = df_clients['client_id'].str.upper()\n",
    "\n",
    "total_clients = len(df_clients)\n",
    "print(f\"\u2713 Loaded {total_clients:,} unique clients\")\n",
    "print(f\"  Columns from client_context: {len(df_clients.columns)}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 2: LOAD CLIENT_INVESTMENT DATA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 2: Loading core.client_investment...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_investment = \"\"\"\n",
    "SELECT \n",
    "    client_id,\n",
    "    time_key,\n",
    "    portfolio_id,\n",
    "    security_name,\n",
    "    asset_class,\n",
    "    security_category,\n",
    "    cost_value_aed,\n",
    "    market_value_aed,\n",
    "    overall_portfolio_xirr_since_inception\n",
    "FROM core.client_investment\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_investment = pd.read_sql(query_investment, engine)\n",
    "    df_investment['client_id'] = df_investment['client_id'].str.upper()\n",
    "    \n",
    "    # Aggregate by client (take latest time_key, sum values)\n",
    "    df_investment_agg = df_investment.sort_values('time_key', ascending=False).groupby('client_id').agg({\n",
    "        'portfolio_id': 'first',\n",
    "        'cost_value_aed': 'sum',\n",
    "        'market_value_aed': 'sum',\n",
    "        'overall_portfolio_xirr_since_inception': 'first',\n",
    "        'security_name': 'count'  # count of holdings\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_investment_agg.columns = ['client_id', 'portfolio_id', 'total_cost_value_aed', \n",
    "                                   'total_market_value_aed', 'portfolio_xirr', 'investment_holdings_count']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_investment):,} investment records\")\n",
    "    print(f\"  Unique clients with investments: {df_investment['client_id'].nunique():,}\")\n",
    "    print(f\"  Aggregated to client level: {len(df_investment_agg)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading client_investment: {e}\")\n",
    "    df_investment_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 3: LOAD CLIENT_PORTFOLIO DATA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 3: Loading core.client_portfolio...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_portfolio = \"\"\"\n",
    "SELECT \n",
    "    client_id,\n",
    "    last_valuation_date,\n",
    "    aum,\n",
    "    investible_cash,\n",
    "    deposits,\n",
    "    asset_distribution\n",
    "FROM core.client_portfolio\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_portfolio = pd.read_sql(query_portfolio, engine)\n",
    "    df_portfolio['client_id'] = df_portfolio['client_id'].str.upper()\n",
    "    \n",
    "    # Take most recent record per client\n",
    "    df_portfolio_latest = df_portfolio.sort_values('last_valuation_date', ascending=False).groupby('client_id').first().reset_index()\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_portfolio):,} portfolio records\")\n",
    "    print(f\"  Unique clients with portfolio: {df_portfolio['client_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading client_portfolio: {e}\")\n",
    "    df_portfolio_latest = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 4: LOAD PRODUCTBALANCE (using customer_number)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 4: Loading core.productbalance (maps to customer_number)...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_productbalance = \"\"\"\n",
    "SELECT \n",
    "    customer_number,\n",
    "    product_description,\n",
    "    product_levl1_desc,\n",
    "    product_levl2_desc,\n",
    "    product_levl3_desc,\n",
    "    outstanding,\n",
    "    account_number,\n",
    "    time_key,\n",
    "    maturity_date\n",
    "FROM core.productbalance\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_productbalance = pd.read_sql(query_productbalance, engine)\n",
    "    df_productbalance['customer_number'] = df_productbalance['customer_number'].str.upper()\n",
    "    \n",
    "    # Aggregate by customer\n",
    "    df_pb_agg = df_productbalance.groupby('customer_number').agg({\n",
    "        'outstanding': 'sum',\n",
    "        'account_number': 'count',\n",
    "        'product_levl1_desc': lambda x: ', '.join(x.unique()[:3])  # top 3 product types\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_pb_agg.columns = ['client_id', 'total_outstanding_balance', 'product_count', 'product_types']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_productbalance):,} product balance records\")\n",
    "    print(f\"  Unique customers with products: {df_productbalance['customer_number'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading productbalance: {e}\")\n",
    "    df_pb_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 5: LOAD CLIENT_PROD_BALANCE_MONTHLY"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 5: Loading core.client_prod_balance_monthly...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_monthly = \"\"\"\n",
    "SELECT \n",
    "    client_id,\n",
    "    year_cal,\n",
    "    month_cal,\n",
    "    closing_current_account_bal,\n",
    "    closing_saving_account_bal\n",
    "FROM core.client_prod_balance_monthly\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_monthly = pd.read_sql(query_monthly, engine)\n",
    "    df_monthly['client_id'] = df_monthly['client_id'].str.upper()\n",
    "    \n",
    "    # Get most recent month per client\n",
    "    df_monthly['year_cal'] = pd.to_numeric(df_monthly['year_cal'], errors='coerce')\n",
    "    df_monthly['month_cal'] = pd.to_numeric(df_monthly['month_cal'], errors='coerce')\n",
    "    df_monthly = df_monthly.sort_values(['year_cal', 'month_cal'], ascending=False)\n",
    "    \n",
    "    df_monthly_latest = df_monthly.groupby('client_id').first().reset_index()\n",
    "    df_monthly_latest['total_casa_balance'] = (\n",
    "        pd.to_numeric(df_monthly_latest['closing_current_account_bal'], errors='coerce').fillna(0) + \n",
    "        pd.to_numeric(df_monthly_latest['closing_saving_account_bal'], errors='coerce').fillna(0)\n",
    "    )\n",
    "    \n",
    "    df_monthly_latest = df_monthly_latest[['client_id', 'total_casa_balance', 'closing_current_account_bal', 'closing_saving_account_bal']]\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_monthly):,} monthly balance records\")\n",
    "    print(f\"  Unique clients with monthly data: {df_monthly['client_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading client_prod_balance_monthly: {e}\")\n",
    "    df_monthly_latest = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 6: LOAD AECB ALERTS (using cif)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 6: Loading core.aecbalerts (maps to cif)...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_aecb = \"\"\"\n",
    "SELECT \n",
    "    cif,\n",
    "    totalamount,\n",
    "    overdueamount,\n",
    "    description_1,\n",
    "    time_key\n",
    "FROM core.aecbalerts\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_aecb = pd.read_sql(query_aecb, engine)\n",
    "    df_aecb['cif'] = df_aecb['cif'].str.upper()\n",
    "    \n",
    "    # Aggregate by client\n",
    "    df_aecb_agg = df_aecb.groupby('cif').agg({\n",
    "        'totalamount': 'sum',\n",
    "        'overdueamount': 'sum',\n",
    "        'description_1': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_aecb_agg.columns = ['client_id', 'aecb_total_amount', 'aecb_overdue_amount', 'aecb_alerts_count']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_aecb):,} AECB alert records\")\n",
    "    print(f\"  Unique clients with AECB alerts: {df_aecb['cif'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading aecbalerts: {e}\")\n",
    "    df_aecb_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 7: LOAD DEBIT TRANSACTIONS (using cif2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 7: Loading core.clienttransactiondebit (maps to cif2)...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_debit = \"\"\"\n",
    "SELECT \n",
    "    cif2,\n",
    "    transactiondate,\n",
    "    transactionamount,\n",
    "    merchantname,\n",
    "    mcc\n",
    "FROM core.clienttransactiondebit\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_debit = pd.read_sql(query_debit, engine)\n",
    "    df_debit['cif2'] = df_debit['cif2'].str.upper()\n",
    "    \n",
    "    # Aggregate by client\n",
    "    df_debit_agg = df_debit.groupby('cif2').agg({\n",
    "        'transactionamount': ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_debit_agg.columns = ['client_id', 'debit_total_amount', 'debit_txn_count', 'debit_avg_amount']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_debit):,} debit transaction records\")\n",
    "    print(f\"  Unique clients with debit transactions: {df_debit['cif2'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading clienttransactiondebit: {e}\")\n",
    "    df_debit_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 8: LOAD CREDIT TRANSACTIONS (using cif2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 8: Loading core.clienttransactioncredit (maps to cif2)...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_credit = \"\"\"\n",
    "SELECT \n",
    "    cif2,\n",
    "    transactiondate,\n",
    "    transactionamount,\n",
    "    merchantname,\n",
    "    mcc\n",
    "FROM core.clienttransactioncredit\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_credit = pd.read_sql(query_credit, engine)\n",
    "    df_credit['cif2'] = df_credit['cif2'].str.upper()\n",
    "    \n",
    "    # Aggregate by client\n",
    "    df_credit_agg = df_credit.groupby('cif2').agg({\n",
    "        'transactionamount': ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_credit_agg.columns = ['client_id', 'credit_total_amount', 'credit_txn_count', 'credit_avg_amount']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_credit):,} credit transaction records\")\n",
    "    print(f\"  Unique clients with credit transactions: {df_credit['cif2'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading clienttransactioncredit: {e}\")\n",
    "    df_credit_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 9: LOAD BANCASSURANCE DATA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 9: Loading core.bancaclientproduct...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_banca = \"\"\"\n",
    "SELECT \n",
    "    client_id,\n",
    "    policy_number,\n",
    "    policy_type,\n",
    "    mkt_val_aed\n",
    "FROM core.bancaclientproduct\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_banca = pd.read_sql(query_banca, engine)\n",
    "    df_banca['client_id'] = df_banca['client_id'].str.upper()\n",
    "    \n",
    "    # Aggregate by client\n",
    "    df_banca_agg = df_banca.groupby('client_id').agg({\n",
    "        'policy_number': 'count',\n",
    "        'mkt_val_aed': 'sum',\n",
    "        'policy_type': lambda x: ', '.join(x.unique()[:3])\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_banca_agg.columns = ['client_id', 'banca_policy_count', 'banca_total_value', 'banca_policy_types']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_banca):,} bancassurance records\")\n",
    "    print(f\"  Unique clients with bancassurance: {df_banca['client_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading bancaclientproduct: {e}\")\n",
    "    df_banca_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 10: LOAD UPSELL OPPORTUNITIES"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 10: Loading app.upsellopportunity...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_upsell = \"\"\"\n",
    "SELECT \n",
    "    client_id,\n",
    "    category,\n",
    "    current_value,\n",
    "    potential_value,\n",
    "    delta,\n",
    "    priority,\n",
    "    status\n",
    "FROM app.upsellopportunity\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_upsell = pd.read_sql(query_upsell, engine)\n",
    "    df_upsell['client_id'] = df_upsell['client_id'].str.upper()\n",
    "    \n",
    "    # Aggregate by client\n",
    "    df_upsell_agg = df_upsell.groupby('client_id').agg({\n",
    "        'delta': 'sum',\n",
    "        'category': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_upsell_agg.columns = ['client_id', 'total_upsell_opportunity', 'upsell_categories_count']\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_upsell):,} upsell opportunity records\")\n",
    "    print(f\"  Unique clients with upsell opportunities: {df_upsell['client_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading upsellopportunity: {e}\")\n",
    "    df_upsell_agg = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 11: LOAD USER JOIN CLIENT CONTEXT (RM MAPPING)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 11: Loading core.user_join_client_context...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "query_rm = \"\"\"\n",
    "SELECT \n",
    "    client_id,\n",
    "    rm_id\n",
    "FROM core.user_join_client_context\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_rm = pd.read_sql(query_rm, engine)\n",
    "    df_rm['client_id'] = df_rm['client_id'].str.upper()\n",
    "    \n",
    "    # Take first RM per client (in case of duplicates)\n",
    "    df_rm_unique = df_rm.groupby('client_id').first().reset_index()\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(df_rm):,} RM-client mapping records\")\n",
    "    print(f\"  Unique clients with RM assigned: {df_rm['client_id'].nunique():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading user_join_client_context: {e}\")\n",
    "    df_rm_unique = pd.DataFrame(columns=['client_id'])\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 12: MERGE ALL DATA USING LEFT JOIN ON CLIENT_ID"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 12: MERGING ALL DATA TABLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Start with base clients table\n",
    "df_merged = df_clients.copy()\n",
    "print(f\"Starting with base: {len(df_merged):,} clients, {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge investment data\n",
    "if not df_investment_agg.empty:\n",
    "    df_merged = df_merged.merge(df_investment_agg, on='client_id', how='left')\n",
    "    print(f\"After merging investment: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge portfolio data\n",
    "if not df_portfolio_latest.empty:\n",
    "    df_merged = df_merged.merge(df_portfolio_latest, on='client_id', how='left')\n",
    "    print(f\"After merging portfolio: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge product balance\n",
    "if not df_pb_agg.empty:\n",
    "    df_merged = df_merged.merge(df_pb_agg, on='client_id', how='left')\n",
    "    print(f\"After merging product balance: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge monthly balance\n",
    "if not df_monthly_latest.empty:\n",
    "    df_merged = df_merged.merge(df_monthly_latest, on='client_id', how='left')\n",
    "    print(f\"After merging monthly balance: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge AECB alerts\n",
    "if not df_aecb_agg.empty:\n",
    "    df_merged = df_merged.merge(df_aecb_agg, on='client_id', how='left')\n",
    "    print(f\"After merging AECB alerts: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge debit transactions\n",
    "if not df_debit_agg.empty:\n",
    "    df_merged = df_merged.merge(df_debit_agg, on='client_id', how='left')\n",
    "    print(f\"After merging debit transactions: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge credit transactions\n",
    "if not df_credit_agg.empty:\n",
    "    df_merged = df_merged.merge(df_credit_agg, on='client_id', how='left')\n",
    "    print(f\"After merging credit transactions: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge bancassurance\n",
    "if not df_banca_agg.empty:\n",
    "    df_merged = df_merged.merge(df_banca_agg, on='client_id', how='left')\n",
    "    print(f\"After merging bancassurance: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge upsell opportunities\n",
    "if not df_upsell_agg.empty:\n",
    "    df_merged = df_merged.merge(df_upsell_agg, on='client_id', how='left')\n",
    "    print(f\"After merging upsell opportunities: {len(df_merged.columns)} columns\")\n",
    "\n",
    "# Merge RM mapping\n",
    "if not df_rm_unique.empty:\n",
    "    df_merged = df_merged.merge(df_rm_unique, on='client_id', how='left')\n",
    "    print(f\"After merging RM mapping: {len(df_merged.columns)} columns\")\n",
    "\n",
    "print(f\"\\n\u2713 Final merged dataset: {len(df_merged):,} clients, {len(df_merged.columns)} total columns\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 13: ANALYZE DATA COVERAGE AND MISSING VALUES"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 13: ANALYZING DATA COVERAGE AND MISSING VALUES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate missing values per column\n",
    "missing_analysis = []\n",
    "\n",
    "for col in df_merged.columns:\n",
    "    total_count = len(df_merged)\n",
    "    non_null_count = df_merged[col].notna().sum()\n",
    "    null_count = total_count - non_null_count\n",
    "    coverage_pct = (non_null_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    missing_analysis.append({\n",
    "        'column_name': col,\n",
    "        'total_clients': total_count,\n",
    "        'clients_with_data': non_null_count,\n",
    "        'clients_missing_data': null_count,\n",
    "        'coverage_pct': round(coverage_pct, 2),\n",
    "        'missing_pct': round(100 - coverage_pct, 2)\n",
    "    })\n",
    "\n",
    "df_coverage = pd.DataFrame(missing_analysis)\n",
    "df_coverage = df_coverage.sort_values('coverage_pct', ascending=True)\n",
    "\n",
    "print(f\"\u2713 Analyzed {len(df_coverage)} columns\")\n",
    "print(f\"  Columns with 100% coverage: {len(df_coverage[df_coverage['coverage_pct'] == 100])}\")\n",
    "print(f\"  Columns with <50% coverage: {len(df_coverage[df_coverage['coverage_pct'] < 50])}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 14: CATEGORIZE COLUMNS BY SOURCE TABLE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 14: Categorizing columns by source table...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Map columns to source tables\n",
    "column_source_map = []\n",
    "\n",
    "# Client context columns (base table)\n",
    "for col in df_clients.columns:\n",
    "    column_source_map.append({\n",
    "        'column_name': col,\n",
    "        'source_table': 'core.client_context',\n",
    "        'is_base_table': True\n",
    "    })\n",
    "\n",
    "# Investment columns\n",
    "for col in df_investment_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.client_investment',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Portfolio columns\n",
    "for col in df_portfolio_latest.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.client_portfolio',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Product balance columns\n",
    "for col in df_pb_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.productbalance',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Monthly balance columns\n",
    "for col in df_monthly_latest.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.client_prod_balance_monthly',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# AECB columns\n",
    "for col in df_aecb_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.aecbalerts',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Debit transaction columns\n",
    "for col in df_debit_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.clienttransactiondebit',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Credit transaction columns\n",
    "for col in df_credit_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.clienttransactioncredit',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Bancassurance columns\n",
    "for col in df_banca_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.bancaclientproduct',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# Upsell columns\n",
    "for col in df_upsell_agg.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'app.upsellopportunity',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "# RM mapping columns\n",
    "for col in df_rm_unique.columns:\n",
    "    if col != 'client_id':\n",
    "        column_source_map.append({\n",
    "            'column_name': col,\n",
    "            'source_table': 'core.user_join_client_context',\n",
    "            'is_base_table': False\n",
    "        })\n",
    "\n",
    "df_column_sources = pd.DataFrame(column_source_map)\n",
    "\n",
    "# Merge with coverage analysis\n",
    "df_coverage_detailed = df_coverage.merge(df_column_sources, on='column_name', how='left')\n",
    "df_coverage_detailed['source_table'] = df_coverage_detailed['source_table'].fillna('Unknown')\n",
    "\n",
    "print(f\"\u2713 Categorized {len(df_coverage_detailed)} columns by source table\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 15: GENERATE SUMMARY STATISTICS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 15: Generating summary statistics...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "summary_stats = {\n",
    "    'total_unique_clients': total_clients,\n",
    "    'total_columns_in_merged_data': len(df_merged.columns),\n",
    "    'columns_from_client_context': len([c for c in df_coverage_detailed['source_table'] if c == 'core.client_context']),\n",
    "    'columns_with_100pct_coverage': len(df_coverage[df_coverage['coverage_pct'] == 100]),\n",
    "    'columns_with_50_to_100pct_coverage': len(df_coverage[(df_coverage['coverage_pct'] >= 50) & (df_coverage['coverage_pct'] < 100)]),\n",
    "    'columns_with_less_than_50pct_coverage': len(df_coverage[df_coverage['coverage_pct'] < 50]),\n",
    "    'average_coverage_pct': round(df_coverage['coverage_pct'].mean(), 2),\n",
    "    'median_coverage_pct': round(df_coverage['coverage_pct'].median(), 2),\n",
    "}\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"-\" * 100)\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 16: IDENTIFY KEY MISSING DATA ISSUES"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 16: Identifying key missing data issues...\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Find columns with significant missing data (>50% missing)\n",
    "high_missing = df_coverage_detailed[df_coverage_detailed['missing_pct'] > 50].sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(f\"Columns with >50% missing data: {len(high_missing)}\")\n",
    "if len(high_missing) > 0:\n",
    "    print(\"\\nTop 10 columns with highest missing data:\")\n",
    "    print(\"-\" * 100)\n",
    "    for idx, row in high_missing.head(10).iterrows():\n",
    "        print(f\"  {row['column_name']:<40} {row['source_table']:<40} {row['missing_pct']:.1f}% missing\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 17: EXPORT ALL RESULTS TO EXCEL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 17: EXPORTING RESULTS TO EXCEL\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = OUTPUT_DIR / f\"EliteX_Data_Quality_Report_{timestamp}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    \n",
    "    # Sheet 1: Executive Summary\n",
    "    df_summary = pd.DataFrame([summary_stats]).T\n",
    "    df_summary.columns = ['Value']\n",
    "    df_summary.index.name = 'Metric'\n",
    "    df_summary.to_excel(writer, sheet_name='Executive Summary')\n",
    "    print(\"\u2713 Sheet 1: Executive Summary\")\n",
    "    \n",
    "    # Sheet 2: All Clients (first 10,000 to avoid size issues)\n",
    "    df_merged.head(10000).to_excel(writer, sheet_name='All Clients (Sample)', index=False)\n",
    "    print(f\"\u2713 Sheet 2: All Clients (Sample - first {min(10000, len(df_merged))} rows)\")\n",
    "    \n",
    "    # Sheet 3: Column Coverage Analysis\n",
    "    df_coverage_detailed.to_excel(writer, sheet_name='Column Coverage Analysis', index=False)\n",
    "    print(f\"\u2713 Sheet 3: Column Coverage Analysis ({len(df_coverage_detailed)} columns)\")\n",
    "    \n",
    "    # Sheet 4: High Missing Data (>50%)\n",
    "    if not high_missing.empty:\n",
    "        high_missing.to_excel(writer, sheet_name='High Missing Data', index=False)\n",
    "        print(f\"\u2713 Sheet 4: High Missing Data ({len(high_missing)} columns)\")\n",
    "    \n",
    "    # Sheet 5: Coverage by Source Table\n",
    "    table_summary = df_coverage_detailed.groupby('source_table').agg({\n",
    "        'column_name': 'count',\n",
    "        'coverage_pct': 'mean'\n",
    "    }).reset_index()\n",
    "    table_summary.columns = ['source_table', 'column_count', 'avg_coverage_pct']\n",
    "    table_summary = table_summary.sort_values('avg_coverage_pct', ascending=False)\n",
    "    table_summary.to_excel(writer, sheet_name='Coverage by Table', index=False)\n",
    "    print(f\"\u2713 Sheet 5: Coverage by Table ({len(table_summary)} tables)\")\n",
    "\n",
    "print(f\"\\n\u2713 Excel report saved: {output_file}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 18: PRINT FINAL SUMMARY TO CONSOLE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nTotal Unique Clients: {total_clients:,}\")\n",
    "print(f\"Total Columns in Merged Dataset: {len(df_merged.columns)}\")\n",
    "print(f\"Average Column Coverage: {summary_stats['average_coverage_pct']}%\")\n",
    "print(f\"Columns with <50% Coverage: {summary_stats['columns_with_less_than_50pct_coverage']}\")\n",
    "\n",
    "print(f\"\\n\u2713 Full report saved to: {output_file}\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"Analysis finished successfully!\")\n",
    "print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}